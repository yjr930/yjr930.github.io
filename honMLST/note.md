### CH1  

**P9**  
注解：在机器学习中，一个属性就是一个数据类型（例如，“里程数”），取决于具体问题一个特征会有多个含义，但通常是属性加上它的值（例如，“里程数=15000”）。许多人是不区分地使用属性和特征。

### CH2
**P39**  
向量的K阶闵氏范数 (可以用于评估距离)  
k=0   汉明范数(即不同的个数)  
k=1   MAE  
k=2   RMSE  
k=inf 切比雪夫范数(即最大值)  

**P52**  
分割数据集时要考虑属性分布进行分层采样。

**P58**  
通过可视化找可能的规律，以及数据异常。
这张图也呈现了一些不是那么明显的直线：一条位于 450000 美元的直线，一条位于 350000 美元的直线，一条在 280000 美元的线，和一些更靠下的线。你可能希望去除对应的街区，以防止算法重复这些巧合。

### CH3
使用混淆矩阵，评估Y=y1被判断成y2的情况，尤其是多分类的问题。  
计算Precision, Recall  

#### PR与ROC  
PR: Precision随着Recall增高而降低；越靠右上(高P高R)越好  
ROC：TPR/FPR, TPR越高，FPR也越高；越靠左上越好(高TPR低FPR)  
使用PR：正例比例少，如检索型问题；其他时候使用ROC  

单阈值分类器属性： 随着阈值增大，Recall会单调递减，Precision会呈升高趋势但并不单调；  

OvA和OvO选择：OvA简单，但OvO需要的数据少，适合向SVM这种数据规模难拓展的。  

`SGDClassifier`:  
    -> 获得分数(decision_function) -> 给定阈值预测(threshold, prediction)

`RandomForestClassifier`  
    -> 给定值域的概率分布(predict_proba) -> 给出概率最高的结果

### CH4
线性回归 => 1 解正规方程(特征维度高时复杂度太高 n2.4-n3); 2 梯度下降(n)

学习率：  
特征尺度归一化会提高收敛速度  
SGD中逐步降低学习率(学习率随epoch呈双曲函数)  

如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了。这种方法可以告诉我们，你的模型是太复杂还是太简单了。

观察学习曲线：在**训练集**和**验证集**上**损失函数**随**训练集大小**的变化

偏差、方差、不可约误差

#### LR回归  
线性回归+sigmoid函数输出概率  
对数损失函数 + Sigmoid函数 使得求导容易，损失函数为`x_j * (y-y')`

多类别LR回归：  
Softmax, 损失函数为交叉熵

### CH5
SVM：在满足间隔的条件下，间隔越宽越好  
wxy>=1-c  
minimize 1/2 * w * wT

Hinge Loss: 即1-wxy  不算C

RBF特征即表示样本与特征点的距离

SVM复杂度：  
无核函数 O(mn)
有核函数 O(m2 * n) ~ O(m3 * n)

SVM回归：逆转SVM分类的目标函数：令最多的样本在支持向量的间隔之内

### CH6
决策树：贪心，每次挑一个最好的特征，按成本函数最小的划分。  
成本函数：Gini IG
CART 对数据的旋转敏感，因为一次只能判断一维特征；用PCA改进

### CH7
硬投票；软投票  

投票：训练不同的分类器，投票  

随机化选择特征(而不是选最好的特征)

#### Bagging/Pasting
同一分类器，使用数据的不同抽样来训练；  
Bagging 有放回; Pasting 无放回  
集成结果的**偏差**不差于单分类器，**方差**低于较单分类器  
容易并行拓展  
除了在数据维度抽样，也可以在特征维度抽样(随机贴片，适用特征维数很大，如图片)  

#### Boosting
串行

AdaBoost  
每次都给上一次错误的样本提权

GradientBoost  
每次拟合上一次的残差

Stacking  
把训练集分为两部分，第一部分训练各个基分类器，第二部分(保持集)训练基分类器的合成(blending)

### CH8
PCA: 最大程度保留数据的方差  
保留到累积方差解释率>K%的维度，可以取K=95  
使用SVD计算主成份


